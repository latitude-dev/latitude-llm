import {
  type EvaluationType,
  LlmEvaluationComparisonSpecification,
  type LlmEvaluationMetric,
} from '@latitude-data/constants'
import { FormFieldGroup } from '@latitude-data/web-ui/atoms/FormFieldGroup'
import type { IconName } from '@latitude-data/web-ui/atoms/Icons'
import { Input } from '@latitude-data/web-ui/atoms/Input'
import { NumberInput } from '@latitude-data/web-ui/atoms/NumberInput'
import { TextArea } from '@latitude-data/web-ui/atoms/TextArea'
import type { ChartConfigurationArgs, ConfigurationFormProps, ResultBadgeProps } from '../index'

const specification = LlmEvaluationComparisonSpecification
export default {
  ...specification,
  icon: 'gitCompareArrows' as IconName,
  ConfigurationSimpleForm: ConfigurationSimpleForm,
  ConfigurationAdvancedForm: ConfigurationAdvancedForm,
  ResultBadge: ResultBadge,
  chartConfiguration: chartConfiguration,
}

function ConfigurationSimpleForm({
  configuration,
  setConfiguration,
  errors,
  disabled,
}: ConfigurationFormProps<EvaluationType.Llm, LlmEvaluationMetric.Comparison>) {
  return (
    <>
      <TextArea
        value={configuration.criteria ?? ''}
        name='criteria'
        label='Criteria'
        description='Instructions to guide the LLM on the criteria to judge against'
        placeholder='Judge the similarity of the translation'
        minRows={2}
        maxRows={4}
        onChange={(e) => setConfiguration({ ...configuration, criteria: e.target.value })}
        errors={errors?.criteria}
        className='w-full'
        disabled={disabled}
        required
      />
      <Input
        value={configuration.passDescription ?? ''}
        name='passDescription'
        label='Pass description'
        description='Additional guidelines describing when the response adequately compares'
        placeholder='The translation is almost identical'
        onChange={(e) =>
          setConfiguration({
            ...configuration,
            passDescription: e.target.value,
          })
        }
        errors={errors?.passDescription}
        className='w-full'
        disabled={disabled}
        required
      />
      <Input
        value={configuration.failDescription ?? ''}
        name='failDescription'
        label='Fail description'
        description='Additional guidelines describing when the response poorly compares'
        placeholder='The translation is completely different'
        onChange={(e) =>
          setConfiguration({
            ...configuration,
            failDescription: e.target.value,
          })
        }
        errors={errors?.failDescription}
        className='w-full'
        disabled={disabled}
        required
      />
    </>
  )
}

function ConfigurationAdvancedForm({
  configuration,
  setConfiguration,
  errors,
  disabled,
}: ConfigurationFormProps<EvaluationType.Llm, LlmEvaluationMetric.Comparison>) {
  return (
    <FormFieldGroup
      layout='horizontal'
      description='The minimum and maximum percentage of criteria met of the response'
    >
      <NumberInput
        value={configuration.minThreshold ?? undefined}
        name='minThreshold'
        label='Minimum threshold'
        placeholder='No minimum'
        min={0}
        max={100}
        onChange={(value) => setConfiguration({ ...configuration, minThreshold: value })}
        errors={errors?.minThreshold}
        defaultAppearance
        className='w-full'
        disabled={disabled}
        required
      />
      <NumberInput
        value={configuration.maxThreshold ?? undefined}
        name='maxThreshold'
        label='Maximum threshold'
        placeholder='No maximum'
        min={0}
        max={100}
        onChange={(value) => setConfiguration({ ...configuration, maxThreshold: value })}
        errors={errors?.maxThreshold}
        defaultAppearance
        className='w-full'
        disabled={disabled}
        required
      />
    </FormFieldGroup>
  )
}

function ResultBadge({
  result,
}: ResultBadgeProps<EvaluationType.Llm, LlmEvaluationMetric.Comparison>) {
  return <>{result.score!.toFixed(0)}% met</>
}

function chartConfiguration({
  evaluation,
}: ChartConfigurationArgs<EvaluationType.Llm, LlmEvaluationMetric.Comparison>) {
  return {
    min: 0,
    max: 100,
    thresholds: {
      lower: evaluation.configuration.minThreshold,
      upper: evaluation.configuration.maxThreshold,
    },
    scale: (point: number) => point,
    format: (point: number, short?: boolean) =>
      short ? `${point.toFixed(0)}%` : `${point.toFixed(0)}% met`,
  }
}
