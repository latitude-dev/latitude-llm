---
title: Ingest traces
description: Learn how to backfill historical LLM traces to Latitude using manual instrumentation
---

## Overview

Use the Telemetry SDK's manual instrumentation to backfill historical LLM logs into Latitude's traces database. This is useful when you have existing logs from LLM calls and want to import them with their original timestamps and structure.

## Span Types

The example demonstrates creating several span types:

- **Prompt span**: The root span representing a prompt execution
- **Completion span**: A child span representing the LLM completion
- **Tool spans**: Child spans representing tool/function calls made by the model

## Timestamp Support

For backfilling historical data, you can specify exact timestamps:

- `startTime`: When the span started (Date object in TypeScript, Unix timestamp in Python)
- `endTime`: When the span ended (passed to the `end()` method)

This allows traces to show accurate durations based on your original log data.

## Span Attributes

### Prompt Span
- `documentLogUuid`: A unique identifier for this log entry (UUID v4)
- `promptUuid`: The path of the prompt in your project
- `projectId`: Your Latitude project ID
- `versionUuid`: The version UUID (use "live" for the published version)
- `template`: The prompt template content
- `parameters`: Parameters used to render the template
- `source`: The source of the log (use `LogSources.API`)
- `startTime`: When the prompt execution started

### Completion Span
- `provider`: The LLM provider name (e.g., "openai", "anthropic")
- `model`: The model name (e.g., "gpt-4o-mini")
- `input`: The input messages array
- `startTime`: When the LLM call started
- `output`: The output messages array (when ending)
- `tokens`: Token usage information
- `finishReason`: The completion finish reason
- `endTime`: When the LLM call completed

### Tool Span
- `name`: The tool/function name
- `call.id`: The unique call ID
- `call.arguments`: The arguments passed to the tool
- `startTime`: When the tool execution started
- `result.value`: The tool result (when ending)
- `result.isError`: Whether the tool call failed
- `endTime`: When the tool execution completed

## Prompt

In this example, the specific prompt content is not important—you just need to have prompts created in a Latitude project with matching paths.

<CodeGroup>
```markdown example
---
provider: openai
model: gpt-4o-mini
---

Tell me a joke about {{topic}}
```
</CodeGroup>

## Code

The example simulates a fake logs database with multiple entries including:
- Simple completions without tools
- Completions with single tool calls (e.g., weather lookup)
- Completions with multiple tool calls (e.g., search + save)

<CodeGroup>
````typescript Typescript
import { Latitude, LogSources } from '@latitude-data/sdk'
import { faker } from '@faker-js/faker'
import { v4 as uuid } from 'uuid'

// ============================================================
// BackfillLog Format - What users send to sdk.traces.manual()
// ============================================================

type BackfillToolCall = {
  id: string
  name: string
  arguments: Record<string, unknown>
  result: unknown
  isError?: boolean
  startedAt: Date
  completedAt: Date
}

type BackfillLog = {
  conversationId: string
  previousTraceId?: string
  path: string
  provider: string
  model: string
  input: Array<{ role: string; content: string }>
  output: string
  promptTokens?: number
  completionTokens?: number
  toolCalls?: BackfillToolCall[]
  startedAt: Date
  completedAt: Date
}

type ManualTracesParams = {
  projectId: number
  versionUuid?: string
  spans: BackfillLog[]
}

type ManualTracesResult = {
  traces: Array<{
    conversationId: string
    traceId: string
  }>
}

// ============================================================
// Example 1: Single turn
// ============================================================

const singleTurnLogs: BackfillLog[] = [
  {
    conversationId: uuid(),
    path: 'customer-support/greeting',
    provider: 'openai',
    model: 'gpt-4o-mini',
    input: [{ role: 'user', content: 'Hello, I need help with my order' }],
    output: 'Hi! I\'d be happy to help you with your order. Could you please provide your order number?',
    promptTokens: 25,
    completionTokens: 30,
    startedAt: new Date('2025-02-10T10:00:00Z'),
    completedAt: new Date('2025-02-10T10:00:01.500Z'),
  },
]

// ============================================================
// Example 2: Multi-turn conversation (ordered by time)
// ============================================================

const conversationId = uuid()

const multiTurnLogs: BackfillLog[] = [
  // Turn 1 - no previousTraceId
  {
    conversationId,
    path: 'assistant/weather',
    provider: 'openai',
    model: 'gpt-4o',
    input: [{ role: 'user', content: "What's the weather in Paris?" }],
    output: 'The weather in Paris is currently 22°C with sunny skies.',
    promptTokens: 15,
    completionTokens: 20,
    toolCalls: [
      {
        id: `call_${faker.string.alphanumeric(24)}`,
        name: 'get_weather',
        arguments: { location: 'Paris', units: 'celsius' },
        result: { temperature: 22, condition: 'sunny' },
        startedAt: new Date('2025-02-10T11:00:00.100Z'),
        completedAt: new Date('2025-02-10T11:00:00.400Z'),
      },
    ],
    startedAt: new Date('2025-02-10T11:00:00Z'),
    completedAt: new Date('2025-02-10T11:00:01.200Z'),
  },
  // Turn 2 - previousTraceId will be set by gateway (same conversationId, ordered by startedAt)
  {
    conversationId,
    path: 'assistant/weather',
    provider: 'openai',
    model: 'gpt-4o',
    input: [{ role: 'user', content: 'What about tomorrow?' }],
    output: 'Tomorrow in Paris will be slightly cooler at 18°C with partly cloudy skies.',
    promptTokens: 12,
    completionTokens: 25,
    toolCalls: [
      {
        id: `call_${faker.string.alphanumeric(24)}`,
        name: 'get_weather_forecast',
        arguments: { location: 'Paris', date: 'tomorrow' },
        result: { temperature: 18, condition: 'partly_cloudy' },
        startedAt: new Date('2025-02-10T11:01:00.100Z'),
        completedAt: new Date('2025-02-10T11:01:00.350Z'),
      },
    ],
    startedAt: new Date('2025-02-10T11:01:00Z'),
    completedAt: new Date('2025-02-10T11:01:01.000Z'),
  },
  // Turn 3 - no tool calls
  {
    conversationId,
    path: 'assistant/weather',
    provider: 'openai',
    model: 'gpt-4o',
    input: [{ role: 'user', content: 'Should I bring an umbrella?' }],
    output: "Based on the forecast, you probably won't need an umbrella tomorrow.",
    promptTokens: 10,
    completionTokens: 35,
    startedAt: new Date('2025-02-10T11:02:00Z'),
    completedAt: new Date('2025-02-10T11:02:00.800Z'),
  },
]

// ============================================================
// Example 3: Multiple independent conversations in one call
// ============================================================

const conversation1Id = uuid()
const conversation2Id = uuid()

const mixedLogs: BackfillLog[] = [
  // Conversation 1, Turn 1
  {
    conversationId: conversation1Id,
    path: 'support/billing',
    provider: 'openai',
    model: 'gpt-4o-mini',
    input: [{ role: 'user', content: 'I have a billing question' }],
    output: 'I can help with billing. What would you like to know?',
    promptTokens: 10,
    completionTokens: 15,
    startedAt: new Date('2025-02-10T09:00:00Z'),
    completedAt: new Date('2025-02-10T09:00:00.800Z'),
  },
  // Conversation 2, Turn 1
  {
    conversationId: conversation2Id,
    path: 'support/technical',
    provider: 'anthropic',
    model: 'claude-3-5-sonnet-20241022',
    input: [{ role: 'user', content: 'My API calls are failing' }],
    output: 'Let me help you debug that. What error are you seeing?',
    promptTokens: 12,
    completionTokens: 18,
    startedAt: new Date('2025-02-10T09:05:00Z'),
    completedAt: new Date('2025-02-10T09:05:01.200Z'),
  },
  // Conversation 1, Turn 2
  {
    conversationId: conversation1Id,
    path: 'support/billing',
    provider: 'openai',
    model: 'gpt-4o-mini',
    input: [{ role: 'user', content: 'Why was I charged twice?' }],
    output: 'Let me look into that for you. Can you provide your account email?',
    promptTokens: 15,
    completionTokens: 20,
    startedAt: new Date('2025-02-10T09:01:00Z'),
    completedAt: new Date('2025-02-10T09:01:01.000Z'),
  },
  // Conversation 2, Turn 2
  {
    conversationId: conversation2Id,
    path: 'support/technical',
    provider: 'anthropic',
    model: 'claude-3-5-sonnet-20241022',
    input: [{ role: 'user', content: 'Error 429 rate limited' }],
    output: 'That means you\'re hitting rate limits. Try adding exponential backoff.',
    promptTokens: 8,
    completionTokens: 22,
    startedAt: new Date('2025-02-10T09:06:00Z'),
    completedAt: new Date('2025-02-10T09:06:00.900Z'),
  },
]

// ============================================================
// Usage
// ============================================================

const sdk = new Latitude(process.env.LATITUDE_API_KEY!)

async function run() {
  const projectId = Number(process.env.PROJECT_ID)

  // Combine all spans into one array
  const allSpans: BackfillLog[] = [
    ...singleTurnLogs,
    ...multiTurnLogs,
    ...mixedLogs,
  ]

  console.log('='.repeat(50))
  console.log('Backfilling spans to Latitude')
  console.log(`Total spans: ${allSpans.length}`)
  console.log('='.repeat(50))

  // Single call to ingest all spans
  // Gateway groups by conversationId and orders by startedAt
  // to automatically link conversation turns
  const result = await sdk.traces.manual({
    projectId,
    versionUuid: 'live',
    spans: allSpans,
  })

  console.log(`\n✓ Ingested ${result.traces.length} traces`)

  // Group results by conversationId for display
  const byConversation = new Map<string, string[]>()
  result.traces.forEach((t) => {
    const traces = byConversation.get(t.conversationId) || []
    traces.push(t.traceId)
    byConversation.set(t.conversationId, traces)
  })

  console.log(`\nConversations created: ${byConversation.size}`)
  byConversation.forEach((traceIds, convId) => {
    console.log(`  ${convId}: ${traceIds.length} turn(s)`)
  })

  console.log('\n' + '='.repeat(50))
  console.log('Done!')
  console.log('='.repeat(50))
}

run()
````
````python Python
import os
import random
import string
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from typing import Any

from latitude_telemetry import (
    EndCompletionSpanOptions,
    EndSpanOptions,
    EndToolSpanOptions,
    LogSources,
    PromptSpanOptions,
    StartCompletionSpanOptions,
    StartToolSpanOptions,
    Telemetry,
    TelemetryOptions,
    TokenUsage,
    ToolCallInfo,
    ToolResultInfo,
)


@dataclass
class ToolCall:
    id: str
    name: str
    arguments: dict[str, Any]
    result: Any
    started_at: datetime
    completed_at: datetime


@dataclass
class LogEntry:
    id: str
    path: str
    created_at: datetime
    completed_at: datetime
    messages: list[dict[str, str]]
    response: str
    model: str
    provider: str
    prompt_tokens: int
    completion_tokens: int
    tool_calls: list[ToolCall] = field(default_factory=list)


def random_id(length: int = 24) -> str:
    return "".join(random.choices(string.ascii_letters + string.digits, k=length))


def random_date_recent(days: int = 7) -> datetime:
    now = datetime.now(timezone.utc)
    offset = timedelta(seconds=random.randint(0, days * 24 * 3600))
    return now - offset


def random_sentence() -> str:
    words = ["lorem", "ipsum", "dolor", "sit", "amet", "consectetur", "adipiscing", "elit"]
    return " ".join(random.choices(words, k=random.randint(5, 12))).capitalize() + "."


def random_paragraph() -> str:
    return " ".join(random_sentence() for _ in range(random.randint(3, 6)))


# Fake logs database - simulating historical LLM calls
FAKE_LOGS_DB: list[LogEntry] = [
    # Simple completion without tools
    LogEntry(
        id=str(uuid.uuid4()),
        path="customer-support/greeting",
        created_at=random_date_recent(days=7),
        completed_at=datetime.now(timezone.utc),
        messages=[{"role": "user", "content": random_sentence()}],
        response=random_paragraph(),
        model="gpt-4o-mini",
        provider="openai",
        prompt_tokens=random.randint(10, 100),
        completion_tokens=random.randint(20, 200),
    ),
    # Completion with tool calls (weather lookup)
    LogEntry(
        id=str(uuid.uuid4()),
        path="assistant/weather",
        created_at=random_date_recent(days=3),
        completed_at=datetime.now(timezone.utc),
        messages=[{"role": "user", "content": f"What's the weather in {random.choice(['Paris', 'London', 'Tokyo', 'New York'])}?"}],
        response=f"The current weather is {random.randint(15, 35)}°C with {random.choice(['sunny skies', 'partly cloudy', 'light rain'])}",
        model="gpt-4o",
        provider="openai",
        prompt_tokens=random.randint(50, 150),
        completion_tokens=random.randint(30, 100),
        tool_calls=[
            ToolCall(
                id=f"call_{random_id()}",
                name="get_weather",
                arguments={"location": random.choice(["Paris", "London", "Tokyo"]), "units": "celsius"},
                result={"temperature": random.randint(15, 35), "condition": random.choice(["sunny", "cloudy", "rainy"]), "humidity": random.randint(30, 90)},
                started_at=datetime.now(timezone.utc),
                completed_at=datetime.now(timezone.utc),
            ),
        ],
    ),
    # Completion with multiple tool calls (search + database)
    LogEntry(
        id=str(uuid.uuid4()),
        path="assistant/research",
        created_at=random_date_recent(days=1),
        completed_at=datetime.now(timezone.utc),
        messages=[{"role": "user", "content": f"Find information about Acme Corp and save it to my notes"}],
        response=random_paragraph() + " " + random_paragraph(),
        model="claude-3-5-sonnet-20241022",
        provider="anthropic",
        prompt_tokens=random.randint(100, 300),
        completion_tokens=random.randint(150, 400),
        tool_calls=[
            ToolCall(
                id=f"call_{random_id()}",
                name="web_search",
                arguments={"query": "Acme Corp", "max_results": 5},
                result={"results": [{"title": random_sentence(), "url": "https://example.com", "snippet": random_paragraph()}]},
                started_at=datetime.now(timezone.utc),
                completed_at=datetime.now(timezone.utc),
            ),
            ToolCall(
                id=f"call_{random_id()}",
                name="save_note",
                arguments={"title": random_sentence(), "content": random_paragraph(), "tags": ["research", "company"]},
                result={"noteId": str(uuid.uuid4()), "savedAt": datetime.now(timezone.utc).isoformat()},
                started_at=datetime.now(timezone.utc),
                completed_at=datetime.now(timezone.utc),
            ),
        ],
    ),
]

# Fix timestamps to have proper durations
for log in FAKE_LOGS_DB:
    base_duration = random.randint(500, 3000)
    log.completed_at = log.created_at + timedelta(milliseconds=base_duration)

    if log.tool_calls:
        tool_offset = 50
        for tool in log.tool_calls:
            tool_duration = random.randint(100, 500)
            tool.started_at = log.created_at + timedelta(milliseconds=tool_offset)
            tool.completed_at = tool.started_at + timedelta(milliseconds=tool_duration)
            tool_offset += tool_duration + 20


telemetry = Telemetry(
    os.environ["LATITUDE_API_KEY"],
    TelemetryOptions(disable_batch=True),
)


def ingest_log(log: LogEntry, project_id: int):
    duration_ms = (log.completed_at - log.created_at).total_seconds() * 1000
    print(f"\nIngesting log: {log.id}")
    print(f"  Path: {log.path}")
    print(f"  Provider: {log.provider}/{log.model}")
    print(f"  Duration: {duration_ms:.0f}ms")
    if log.tool_calls:
        print(f"  Tool calls: {', '.join(t.name for t in log.tool_calls)}")

    # Create prompt span
    prompt_span = telemetry.span.prompt(
        PromptSpanOptions(
            documentLogUuid=log.id,
            promptUuid=log.path,
            projectId=project_id,
            versionUuid="live",
            template="User message: {{message}}",
            parameters={"message": log.messages[0]["content"] if log.messages else ""},
            source=LogSources.API,
            startTime=log.created_at.timestamp(),
        )
    )

    # Create completion span
    completion_span = telemetry.span.completion(
        StartCompletionSpanOptions(
            provider=log.provider,
            model=log.model,
            input=log.messages,
            startTime=log.created_at.timestamp(),
        ),
        prompt_span.context,
    )

    # Create tool spans if present
    for tool_call in log.tool_calls:
        tool_span = telemetry.span.tool(
            StartToolSpanOptions(
                name=tool_call.name,
                call=ToolCallInfo(id=tool_call.id, arguments=tool_call.arguments),
                startTime=tool_call.started_at.timestamp(),
            ),
            completion_span.context,
        )

        tool_span.end(
            EndToolSpanOptions(
                result=ToolResultInfo(value=tool_call.result, isError=False),
                endTime=tool_call.completed_at.timestamp(),
            )
        )

    # End completion span
    completion_span.end(
        EndCompletionSpanOptions(
            output=[{"role": "assistant", "content": log.response}],
            tokens=TokenUsage(prompt=log.prompt_tokens, completion=log.completion_tokens),
            finishReason="stop",
            endTime=log.completed_at.timestamp(),
        )
    )

    # End prompt span
    prompt_span.end(
        EndSpanOptions(endTime=log.completed_at.timestamp() + 0.005)
    )


def run():
    project_id = int(os.environ["PROJECT_ID"])

    print("=" * 50)
    print("Backfilling historical logs to Latitude")
    print(f"Total logs to ingest: {len(FAKE_LOGS_DB)}")
    print("=" * 50)

    for log in FAKE_LOGS_DB:
        ingest_log(log, project_id)

    telemetry.flush()

    print("\n" + "=" * 50)
    print("All traces backfilled successfully!")
    print("=" * 50)


if __name__ == "__main__":
    run()
````
</CodeGroup>
