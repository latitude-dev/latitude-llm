---
title: 'Contextual Memory'
description: 'Learn how to implement contextual memory for AIs to retain and utilize information across multiple interactions'
---

## What is Contextual Memory?

Contextual memory is a prompting technique that enables AI systems to retain and reference information from previous interactions, creating a more coherent and personalized conversation experience. This technique helps the AI build a cumulative understanding of the conversation or user preferences over time, allowing for more relevant and customized responses.

## Why Use Contextual Memory?

- **Continuity in Conversations**: Maintains context across multiple exchanges
- **Personalization**: Tailors responses based on user preferences and history
- **Reduced Repetition**: Eliminates the need to restate information in each prompt
- **Knowledge Building**: Accumulates relevant information over time
- **Enhanced User Experience**: Creates more natural, human-like interactions

## Basic Implementation in Latitude

Here's a simple contextual memory implementation for a conversational assistant:

```markdown Conversational Memory
---
provider: OpenAI
model: gpt-4o
temperature: 0.5
---

# Conversational Assistant with Memory

I am a helpful assistant that remembers our conversation context.

## Memory:
{{ memory || "No previous interactions yet." }}

## Current Conversation:
User: {{ user_message }}

## Response:
Let me respond based on our conversation history:
```

## Advanced Implementation with Memory Management

Let's create a more sophisticated example that uses Latitude's chain feature to manage and update contextual memory:

<CodeGroup>
```markdown Advanced Contextual Memory
---
provider: OpenAI
model: gpt-4o
temperature: 0.7
---

<step>
# Memory Analysis

Review the existing memory and current query:

## Current Memory:
{{ memory || "No established memory yet." }}

## User Query:
{{ user_query }}

## Analysis:
Analyze what information is relevant from memory and what new information should be retained:

1. **Relevant existing information:**
2. **New information to retain:**
3. **Information that can be discarded:**
</step>

<step as="memory_update" schema={{
  "type": "object",
  "properties": {
    "updated_memory": {
      "type": "string"
    },
    "memory_operations": {
      "type": "array",
      "items": {
        "type": "string"
    }
  }
  },
  "required": ["updated_memory", "memory_operations"]
}}>
# Memory Update

Based on the analysis, construct an updated memory:

## Memory Operations:
- What information was added?
- What information was modified?
- What information was removed?

## Updated Memory:
Provide a concise, structured representation of the important information to retain:
</step>

<step>
# Response Generation

Generate a response using the updated memory context:

## User Query:
{{ user_query }}

## Relevant Memory:
{{ memory_update.updated_memory }}

## Response:
Respond to the user appropriately, incorporating relevant context from memory:
</step>
```
</CodeGroup>

In this advanced example:

1. **Memory Analysis**: We evaluate what parts of existing memory are relevant to the current query
2. **Memory Management**: We decide what to keep, update, or discard from memory
3. **Structured Memory**: We maintain a structured representation of important information
4. **Contextual Response**: We generate a response that incorporates the relevant context

## Conversational Memory with Recency Bias

Implement memory that prioritizes recent interactions but preserves important older information:

<CodeGroup>
```markdown Recency-Based Memory
---
provider: OpenAI
model: gpt-4o
temperature: 0.6
---

<step>
# Memory Prioritization

Evaluate the conversation history and prioritize information:

## Complete Conversation History:
{{ conversation_history }}

## Memory Structure:
1. **Recent Context (Last 3 interactions):**
2. **Important Facts (User preferences, key information):**
3. **General Themes (Overall conversation topics):**

Analyze and extract the most relevant information from each category.
</step>

<step>
# Recency-Weighted Response

Generate a response that emphasizes recent context while incorporating important facts:

## User's Current Query:
{{ current_query }}

## Prioritized Memory:
### Recent Context:
- Most recent exchanges
- Current conversation flow

### Important Facts:
- User preferences
- Key information shared

### General Themes:
- Overall conversation direction

## Response:
Based on the above context, craft a response that feels natural and contextually appropriate:
</step>
```
</CodeGroup>

## Multi-Agent Memory Coordination

Combine contextual memory with Latitude's agent system for specialized knowledge retention:

<CodeGroup>
```markdown Multi-Agent Memory
---
provider: OpenAI
model: gpt-4o
temperature: 0.5
type: agent
agents:
  - agents/profile_manager
  - agents/conversation_tracker
  - agents/knowledge_base
---

# Multi-Agent Memory Coordination

Coordinate memory across specialized agents:

## User Input:
{{ user_input }}

## Memory Coordination:
1. **Profile Manager**: Maintain user preferences and personal details
2. **Conversation Tracker**: Track conversation flow and recent interactions
3. **Knowledge Base**: Maintain factual information shared during conversations

Coordinate with all agents to access relevant memory and provide a coherent response.
```

```markdown agents/profile_manager
---
provider: OpenAI
model: gpt-4o
temperature: 0.4
type: agent
---

# Profile Manager Agent

I maintain user preferences, demographic information, and personal details.

## Current Profile:
{{ user_profile || "No profile established yet." }}

## Profile Update:
Based on the latest interaction, update the user profile:

1. **New preferences detected:**
2. **Modified information:**
3. **Updated profile:**

## Profile Summary:
Provide a concise summary of the most relevant user information for this interaction:
```

```markdown agents/conversation_tracker
---
provider: OpenAI
model: gpt-4o
temperature: 0.4
type: agent
---

# Conversation Tracker Agent

I maintain the flow of conversation, track recent interactions, and identify current context.

## Conversation Flow:
{{ conversation_flow || "No prior conversation." }}

## Context Analysis:
1. **Current conversation topic:**
2. **Recent interaction summary:**
3. **Conversation continuity elements:**

## Conversation Memory:
Provide the most relevant conversational context to maintain coherent dialogue:
```

```markdown agents/knowledge_base
---
provider: OpenAI
model: gpt-4o
temperature: 0.4
type: agent
---

# Knowledge Base Agent

I maintain factual information shared during conversations and relevant external knowledge.

## Knowledge Repository:
{{ knowledge_repository || "No knowledge repository established yet." }}

## Knowledge Update:
1. **New facts detected:**
2. **Facts to verify:**
3. **Knowledge gaps identified:**

## Relevant Knowledge:
Provide the most relevant facts and information for this interaction:
```
</CodeGroup>

## Best Practices for Contextual Memory

<AccordionGroup>
<Accordion title="Memory Management">
**Optimal Storage**:
- Store only relevant, high-value information
- Prioritize recent information but preserve important older context
- Use structured formats (JSON, key-value pairs) for efficient storage
- Implement decay mechanisms for less important information

**Memory Operations**:
- Add new important information
- Update existing information when new details emerge
- Summarize verbose information to save context space
- Prune obsolete or low-value information
</Accordion>

<Accordion title="Memory Retrieval">
**Retrieval Strategies**:
- **Recency**: Prioritize recent interactions
- **Relevance**: Match to current query topic
- **Importance**: Weigh critical user information higher
- **Consistency**: Ensure retrieved memory aligns with conversation flow

**Context Windows**:
- Balance between comprehensive history and focus
- Implement sliding windows of context based on relevance
- Use dynamic windows that expand/contract based on topic complexity
- Summarize distant history while preserving detail in recent exchanges
</Accordion>

<Accordion title="Memory Applications">
**Best Use Cases**:
- Multi-turn conversations requiring continuity
- Personalized assistants that learn user preferences
- Educational applications tracking learning progress
- Customer service bots handling complex issues
- Creative collaborations requiring shared context

**Less Suitable Cases**:
- One-off factual queries
- Privacy-sensitive applications
- When fresh perspective is preferred for each interaction
- Stateless API integrations
</Accordion>

<Accordion title="Performance Optimization">
**Efficiency Tips**:
- Use compression techniques for long-term memory
- Implement tiered memory (working memory, long-term memory)
- Index memory for faster retrieval of relevant information
- Periodically consolidate and summarize memory

**Quality Management**:
- Verify important information before storing
- Implement confidence scores for stored memories
- Distinguish between observed facts and inferred information
- Regularly audit memory for accuracy and relevance
</Accordion>
</AccordionGroup>

## Advanced Techniques

### Hierarchical Memory Architecture

Create a tiered memory system with different levels of persistence:

<CodeGroup>
```markdown Hierarchical Memory
---
provider: OpenAI
model: gpt-4o
temperature: 0.6
---

<step>
# Hierarchical Memory Processing

Process information across different memory tiers:

## User Input:
{{ user_input }}

## Memory Tiers:
1. **Working Memory** (current session):
   {{ working_memory || "No working memory established yet." }}

2. **Short-Term Memory** (recent sessions):
   {{ short_term_memory || "No short-term memory established yet." }}

3. **Long-Term Memory** (persistent important information):
   {{ long_term_memory || "No long-term memory established yet." }}

## Memory Operations:
Determine appropriate operations for each memory tier:
</step>

<step as="memory_updates" schema={{"type": "object", "properties": {"working_memory": {"type": "string"}, "short_term_memory": {"type": "string"}, "long_term_memory": {"type": "string"}}, "required": ["working_memory", "short_term_memory", "long_term_memory"]}}>
# Memory Update Process

Update each memory tier appropriately:

## Working Memory Update:
Detail the most immediately relevant information for this session:

## Short-Term Memory Update:
Update information that should persist across a few sessions:

## Long-Term Memory Update:
Add or modify persistent information about user preferences, important facts, etc.:
</step>

# Response Generation

Generate a response that integrates information from all memory tiers as appropriate:

## Relevant Context from Memory:
- **Working Memory**: {{ memory_updates.working_memory }}
- **Short-Term Context**: {{ memory_updates.short_term_memory }}
- **Long-Term Knowledge**: {{ memory_updates.long_term_memory }}

## Response:
```
</CodeGroup>

### Contextual Memory with Forgotten Information Recovery

Implement memory that can recover "forgotten" information when needed:

<CodeGroup>
```markdown Memory Recovery
---
provider: OpenAI
model: gpt-4o
temperature: 0.7
---

<step>
# Memory Gap Detection

Analyze the current query for references to information that might be in memory but not currently active:

## User Query:
{{ user_query }}

## Active Memory:
{{ active_memory }}

## Possible Memory Gaps:
Identify references to information that might exist in archived memory:
</step>

<step as="memory_search" schema={{
  "type": "object",
  "properties": {
    "memory_gaps": {
      "type": "array",
       "items": {
          "type": "string"
        }
    },
    "search_needed": {
      "type": "boolean"
    }
  },
  "required": ["memory_gaps", "search_needed"]
}}>

# Memory Search Decision

Determine if archived memory should be searched:

## Identified Memory Gaps:
- List potential information gaps

## Search Decision:
Is a search of archived memory needed? (Yes/No)
Reason:
</step>

{{ if memory_search.search_needed }}
<step as="recovered_memory">
# Memory Recovery

Search archived memory for the missing information:

## Search Terms:
{{ memory_search.memory_gaps | join(", ") }}

## Archived Memory:
{{ archived_memory }}

## Recovered Information:
Extract the relevant information from archived memory:
</step>
{{ endif }}

# Response with Recovered Context

Generate a response incorporating any recovered memory:

## User Query:
{{ user_query }}

## Combined Memory Context:
{{ active_memory }}
{{ if memory_search.search_needed }}
{{ recovered_memory }}
{{ endif }}

## Response:
```
</CodeGroup>

## Integration with Other Techniques

Contextual memory works well combined with other prompting techniques:

- **Chain-of-Thought + Contextual Memory**: Preserve reasoning patterns across interactions
- **Few-Shot Learning + Contextual Memory**: Refine examples based on user interactions
- **Persona + Contextual Memory**: Maintain consistent character while adapting to interactions
- **Self-Consistency + Contextual Memory**: Use memory to validate consistency over time

The key is to maintain relevant context while ensuring the memory management is efficient and focused.

## Related Techniques

Explore these complementary prompting techniques to enhance your AI applications:

### Core Reasoning Techniques
- **[Chain-of-Thought](./chain-of-thought)** - Break down complex problems into step-by-step reasoning
- **[Self-Consistency](./self-consistency)** - Generate multiple reasoning paths for improved reliability
- **[Few-Shot Learning](./few-shot-learning)** - Use examples to guide AI behavior and improve consistency

### Advanced Reasoning Methods
- **[Iterative Refinement](./iterative-refinement)** - Progressively improve answers through multiple iterations
- **[Meta-Prompting](./meta-prompting)** - Use AI to optimize and improve prompts themselves
- **[Tree-of-Thoughts](./tree-of-thoughts)** - Explore multiple reasoning paths systematically

### Collaborative Approaches
- **[Multi-Agent Collaboration](./multi-agent-collaboration)** - Coordinate multiple AI agents for complex tasks
- **[Role Prompting](./role-prompting)** - Assign specific expert roles to improve specialized reasoning
- **[Prompt Chaining](./prompt-chaining)** - Connect multiple prompts for complex workflows

### Quality & Reliability
- **[Constitutional AI](./constitutional-ai)** - Ensure AI responses follow ethical guidelines and principles
- **[Emotional Intelligence Prompting](./emotional-intelligence-prompting)** - Create more empathetic AI responses
