---
title: LLM as Judge
description: 'Learn how to use LLMs as judges to evaluate the quality of your prompts.'
---

## What are LLM as judge evaluations?

LLM as judge evaluations use large language models to evaluate the quality of other LLM responses. This approach is particularly powerful when you need to assess subjective, complex, or flexible criteria that are difficult to evaluate with simple rules or metrics.

## Using LLMs to judge

### Use cases

LLM as judge evaluations are best suited for scenarios where:

- **Subjective Assessment**: You need to evaluate qualities like faithfulness, coherence, or helpfulness that require nuanced understanding
- **Complex Criteria**: The evaluation criteria involve multiple factors or require contextual understanding
- **Flexible Requirements**: The expected output can vary in form or content while still meeting quality standards
- **Natural Language Understanding**: You need to assess how well the response understands and addresses the user's intent

### Trade-offs

While LLM as judge evaluations are powerful, they come with some considerations:

- **Token Usage**: Each evaluation consumes tokens, which can impact your costs
- **Latency**: Evaluations may take longer than programmatic rules
- **Consistency**: Results may vary slightly between runs due to the inherent nature of LLMs

For simpler, objective criteria (like format validation or exact matches), consider using [Programmatic rule evaluations](/guides/evaluations/programmatic_rules) instead.
For user-given feedback or required human verification, consider using [Human-in-the-loop evaluations](/guides/evaluations/manual_evaluations).

## Creating an LLM as judge evaluation

You can either leverage the catalog of built-in [evaluations templates](/guides/evaluations/evaluation-templates) that Latitude comes with, to get started faster; or create your own [custom evaluations](/guides/evaluations/custom-evaluations) from scratch.

To learn more about how to run evaluations, check out the [Running evaluations](/guides/evaluations/running-evaluations) guide.
