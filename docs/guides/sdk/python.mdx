---
title: Python SDK
description: Integrate Latitude into your Python applications using the Python SDK.
---

# Python SDK

The Latitude Python SDK allows you to easily interact with the Latitude platform from your Python applications.

## Installation

```bash
pip install latitude-sdk
```

## Authentication and Initialization

Import the client and initialize it with your API key. Obtain keys from your Latitude project settings under "API Access".

```python
import os
from latitude import LatitudeClient

sdk = Latitude('your-api-key-here', LatitudeOptions(
    project_id=12345, # Optional, otherwise you have to provide it in each method
    version_uuid='optional-version-uuid', # Optional, by default it targets the latest live version
))
```

## Examples

Check out our [cookbook](/guides/cookbook/overview#python) for more examples of how to use Latitude's SDK.

## Prompt Management

### Get or create a prompt

To get or create a prompt, use the `get_or_create` method:

```python
from latitude_sdk import GetOrCreatePromptOptions

await sdk.prompts.get_or_create('path/to/your/prompt', GetOrCreatePromptOptions(
    project_id=12345, # Optional, if you did not provide it in the constructor
    version_uuid='optional-version-uuid', # Optional, by default it targets the latest live version
    prompt='Your prompt here', # Optional, this will be the contents of your prompt if it does not exist
))
```

### Run a prompt with your LLM provider

The `render` method will render your prompt and return the configuration and
messages to use with your LLM provider. This render step is completely local and
does not use Latitude's runtime services:

```python
from latitude_sdk import GetPromptOptions, RenderPromptOptions
from promptl_ai import Adapter
from openai import OpenAI

prompt = await sdk.prompts.get('path/to/your/prompt', GetPromptOptions(
    project_id=12345, # Optional, if you did not provide it in the constructor
    version_uuid='optional-version-uuid', # Optional, by default it targets the latest live version
))

result = await sdk.prompts.render(prompt, RenderPromptOptions(
    parameters={
        # Any parameters your prompt expects
    },
    adapter=Adapter.OpenAI, # Optional, by default is OpenAI
))

response = openai.chat.completions.create(
    **result.config,
    messages=[message.model_dump() for message in result.messages],
)
```

Always use environment variables or secure secrets management for your API key.

## Programmatically Managing Prompts

Interact with your prompts directly from Python.

### Create a Prompt

```python
def create_new_prompt():
    try:
        new_prompt = latitude.prompts.create(
            name="Support Ticket Classifier",
            content="""
            ---
            provider: openai
            model: gpt-4o-mini
            schema:
              type: object
              properties:
                category:
                  type: string
                  enum: [Billing, Technical, Feature Request, Other]
              required: [category]
            ---
            <system>Classify the following support ticket into one of the predefined categories.</system>
            <user>Ticket Text: {{ticket_text}}</user>
            """,
            # project_id="your_project_id" # Optional
        )
        print(f"Created Prompt: {new_prompt.id}")
        return new_prompt
    except Exception as e:
        print(f"Error creating prompt: {e}")
```

### Get a Prompt

```python
def get_prompt(prompt_id):
    try:
        prompt = latitude.prompts.get(prompt_id)
        print(f"Fetched Prompt: {prompt.name}")
        return prompt
    except Exception as e:
        print(f"Error fetching prompt {prompt_id}: {e}")
```

### List Prompts

```python
def list_all_prompts():
    try:
        prompts = latitude.prompts.list()
        print("Prompts:", [(p.id, p.name) for p in prompts])
        return prompts
    except Exception as e:
        print(f"Error listing prompts: {e}")
```

## Running Prompts and Handling Responses

Execute published prompts via the AI Gateway using their ID.

### Standard Run (Non-Streaming)

Returns the full response after generation is complete.

```python
def run_classifier_prompt(prompt_id):
    try:
        response = latitude.run(prompt_id,
            parameters={
                "ticket_text": "I can't log in to my account, the password reset link isn't working."
            },
            metadata={
                "user_id": "user-pqr-456",
                "source": "web-support-form"
            }
        )

        print("Classification Result:")
        print(response.content) # Often a JSON string if schema is used

        # Access response metadata
        if response.metadata:
            print(f"Trace ID: {response.metadata.trace_id}")
            print(f"Log ID: {response.metadata.log_id}")

        return response
    except Exception as e:
        print(f"Error running prompt: {e}")
```

### Handling Streaming Responses

Process response chunks in real-time.

```python
def run_streaming_prompt(prompt_id):
    try:
        stream = latitude.run(prompt_id,
            parameters={
                "ticket_text": "How do I upgrade my subscription?"
            },
            stream=True, # Enable streaming
            metadata={"user_id": "user-stu-789"}
        )

        print("Streaming Response:")
        full_content = ""
        for chunk in stream:
            # Process text chunks
            if chunk.type == 'text-delta' and chunk.content:
                print(chunk.content, end="", flush=True)
                full_content += chunk.content
            # Handle other event types
            if chunk.type == 'response':
                print(f"\n--- Stream Finished --- ")
                if chunk.metadata:
                    print(f"Final Metadata (Trace ID): {chunk.metadata.trace_id}")
            # Add handling for chunk.type == 'tool-call', 'error', etc.
            # See Streaming Events guide for details: /guides/api/streaming-events

        # full_content holds the complete text
    except Exception as e:
        print(f"Error running streaming prompt: {e}")

```

See the [Streaming Events](/guides/api/streaming-events) documentation for details on all event types.

## Logging and Telemetry Features

- **Automatic Logging**: Calls via `latitude.run()` are logged automatically, capturing inputs, outputs, metadata, performance, and traces.
- **Custom Metadata**: Include application context (e.g., `user_id`, `session_id`) via the `metadata` argument in the `run` call to enrich logs.
- **Trace IDs**: Link related events (LLM calls, tool calls) using the `trace_id` from the response metadata.

## Further Information

- [HTTP API Reference](/guides/api/reference)
- [API Access and Authentication](/guides/api/api-access)
- [Streaming Event Details](/guides/api/streaming-events)
