---
title: Quick Start
description: Learn how to install and use PromptL in your project
---

## Overview

PromptL simplifies the process of creating and managing prompts for large language models (LLMs). This quick start guide will show you how to set up PromptL in your project and generate dynamic prompts with minimal effort.

> **Prerequisites**: Ensure you have Node.js installed and access to an LLM provider like OpenAI or Anthropic.

---

## Installation

Install PromptL via npm:

```bash
$ npm install promptl-ai
```

You’ll also need the library for your LLM provider.
<CodeGroup>
```bash OpenAI
$ npm install openai
```
```bash Anthropic
$ npm install @anthropic-ai/sdk
```
</CodeGroup>

---

## Basic Usage

Here’s how to use PromptL to generate a dynamic prompt and interact with an LLM:

<Note>
  Different providers will require a different setup and structure. Check out the [Adapters](/promptl/usage/adapters/overview) section for more information on how to integrate with your provider.
</Note>

### Example Code
```javascript
import { render } from '@latitude-data/promptl'
import OpenAI from 'openai'

// Define your PromptL prompt
const prompt = `
---
model: gpt-4o
temperature: 0.6
---

Generate a joke about {{ topic }}.
`

// Render the prompt with dynamic parameters
const { messages, config } = await render({
  prompt,
  parameters: { topic: 'chickens' },
})

// Initialize your LLM client
const client = new OpenAI()
const response = await client.chat.completions.create({
  ...config,
  messages,
})

// Output the response
console.log(response.choices[0].message.content)
```

### How It Works:
1. **Prompt Definition**: The `prompt` variable defines the PromptL prompt, including configuration and template syntax.
2. **Dynamic Parameters**: The `parameters` object passes the value `topic: 'chickens'` to replace `{{ topic }}` in the prompt.
3. **Rendering**: The `render` function processes the prompt and generates the `messages` array and `config` object for your LLM provider.
4. **LLM Interaction**: The OpenAI client sends the `messages` and `config` to the model, generating a response.

---

## Advanced Example: Error Handling

For production environments, add error handling to manage unexpected issues:

```javascript
import { render } from '@latitude-data/promptl'
import OpenAI from 'openai'

async function generateResponse(prompt, parameters) {
  try {
    const { messages, config } = await render({ prompt, parameters })

    const client = new OpenAI()
    const response = await client.chat.completions.create({
      ...config,
      messages,
    })

    return response.choices[0].message.content
  } catch (error) {
    console.error('Error generating response:', error)
    return 'An error occurred while generating the response.'
  }
}

// Example usage
const prompt = `
---
model: gpt-4o
temperature: 0.6
---

Generate a joke about {{ topic }}.
`

const joke = await generateResponse(prompt, { topic: 'chickens' })
console.log(joke)
```

---

## Next Steps

Once you’ve set up PromptL, explore its advanced features:
- [Syntax and Variables](syntax/variables): Learn how to define dynamic variables in your prompts.
- [Chains and Steps](syntax/chains): Break down complex tasks into multi-step prompts.
- [Prompt References](syntax/prompt-references): Reuse common prompt components across your project.

---

## Summary

PromptL makes it easy to create and manage dynamic prompts for LLMs. By following this guide, you’ve set up PromptL, generated a dynamic prompt, and integrated it with an LLM provider. Now, you’re ready to explore its full potential.
